{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Start--\n",
      "0.9211249370095845\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001} 0.9087271005633317\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_jobs': -1, 'n_estimators': 1000} 0.8757505302628772\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    8.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 2, 'n_estimators': 200} 0.9057338944783676\n",
      "経過時間:390.771213054657[sec]\n",
      "--End--\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression,Lasso\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from google.cloud import bigquery\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#一時的にwarnings非表示に\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "print(\"--Start--\")\n",
    "start = time.time()\n",
    "\n",
    "# データの読み込み\n",
    "train =pd.read_csv('gs://sample_machine_learning_input/HousePrices/train.csv')\n",
    "test = pd.read_csv('gs://sample_machine_learning_input/HousePrices/test.csv')\n",
    "\n",
    "test_id = test['Id']\n",
    "\n",
    "# データタイプがobjectの列の値をラベル化した数値に変換\n",
    "for i in range(train.shape[1]):\n",
    "    if train.iloc[:,i].dtypes == object:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train.iloc[:,i].values) + list(test.iloc[:,i].values))\n",
    "        train.iloc[:,i] = lbl.transform(list(train.iloc[:,i].values))\n",
    "        test.iloc[:,i] = lbl.transform(list(test.iloc[:,i].values))\n",
    "\n",
    "\n",
    "train = train.drop(train[(train['LotArea']>100000)].index)\n",
    "train = train.drop(train[(train['Street']<0.1)].index)\n",
    "train = train.drop(train[(train['Utilities']>0.9)].index)\n",
    "train = train.drop(train[(train['SalePrice']>700000)].index)\n",
    "train = train.drop(train[(train['BsmtFinSF1']>5000)].index)\n",
    "train = train.drop(train[(train['Electrical']>4.5)].index)\n",
    "train = train.drop(train[(train['LowQualFinSF']>560)].index)\n",
    "train = train.drop(train[(train['GrLivArea']>4500)].index)\n",
    "train = train.drop(train[(train['BsmtFullBath']>2.5)].index)\n",
    "train = train.drop(train[(train['BsmtHalfBath']>1.75)].index)\n",
    "train = train.drop(train[(train['BedroomAbvGr']>7)].index)\n",
    "train = train.drop(train[(train['KitchenAbvGr']>2.75)].index)\n",
    "train = train.drop(train[(train['OpenPorchSF']>500)].index)\n",
    "train = train.drop(train[(train['EnclosedPorch']>500)].index)\n",
    "train = train.drop(train[(train['SaleCondition']>-1) & (train['SalePrice']>700000)].index)\n",
    "\n",
    "\n",
    "train = train.drop(['Street','Utilities','Condition2','CentralAir','LowQualFinSF','KitchenAbvGr','PavedDrive','PoolArea','PoolQC','MiscVal'],axis=1)\n",
    "test = test.drop(['Street','Utilities','Condition2','CentralAir','LowQualFinSF','KitchenAbvGr','PavedDrive','PoolArea','PoolQC','MiscVal'],axis=1)\n",
    "\n",
    "\n",
    "total = train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)\n",
    "\n",
    "train = train.drop((missing_data[missing_data['Total'] > 1]).index,1)\n",
    "train = train.drop(train.loc[train['Electrical'].isnull()].index)\n",
    "train.isnull().sum().max()\n",
    "\n",
    "#test = test.drop((missing_data[missing_data['Total'] > 1]).index,1)\n",
    "#test = test.drop(test.loc[test['Electrical'].isnull()].index)\n",
    "test.isnull().sum().max()\n",
    "\n",
    "\n",
    "saleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:,np.newaxis]);\n",
    "low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\n",
    "high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\n",
    "\n",
    "\n",
    "train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n",
    "train = train.drop(train[train['Id'] == 1299].index)\n",
    "train = train.drop(train[train['Id'] == 524].index)\n",
    "\n",
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "train['GrLivArea'] = np.log(train['GrLivArea'])\n",
    "test['GrLivArea'] = np.log(test['GrLivArea'])\n",
    "\n",
    "\n",
    "train['HasBsmt'] = pd.Series(len(train['TotalBsmtSF']), index=train.index)\n",
    "train['HasBsmt'] = 0 \n",
    "train.loc[train['TotalBsmtSF']>0,'HasBsmt'] = 1\n",
    "\n",
    "test['HasBsmt'] = pd.Series(len(test['TotalBsmtSF']), index=test.index)\n",
    "test['HasBsmt'] = 0 \n",
    "test.loc[test['TotalBsmtSF']>0,'HasBsmt'] = 1\n",
    "\n",
    "train.loc[train['HasBsmt']==1,'TotalBsmtSF'] = np.log(train['TotalBsmtSF'])\n",
    "test.loc[test['HasBsmt']==1,'TotalBsmtSF'] = np.log(test['TotalBsmtSF'])\n",
    "\n",
    "Xmat = pd.concat([train, test])  \n",
    "#欠損値の少ないカラムのNaNは中央値(median)で埋める\n",
    "Xmat = Xmat.fillna(Xmat.median())\n",
    "#trainデータとtestデータを含んでいるXmatを、再度trainデータとtestデータに分割\n",
    "train = Xmat.iloc[:train.shape[0],:]\n",
    "test = Xmat.iloc[train.shape[0]:,:]\n",
    "\n",
    "y = train['SalePrice']\n",
    "train = train.drop(['Id','SalePrice'],axis=1)\n",
    "test = test.drop(['Id','SalePrice'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---\n",
    "skl = LinearRegression()\n",
    "skl.fit(train,y)\n",
    "print(skl.score(train,y))\n",
    "\n",
    "\n",
    "#---\n",
    "las_model = Lasso()\n",
    "las_cv = GridSearchCV(las_model, {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001]}, verbose=1)\n",
    "las_cv.fit(train, y)\n",
    "print(las_cv.best_params_, las_cv.best_score_)\n",
    "# 改めて最適パラメータで学習\n",
    "las_reg = Lasso(**las_cv.best_params_)\n",
    "las_reg.fit(train, y)\n",
    "\n",
    "#---\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_cv = GridSearchCV(forest_model, {'n_estimators'  : [3, 10, 100, 1000, 10000], 'n_jobs': [-1]}, verbose=1)\n",
    "forest_cv.fit(train, y)\n",
    "print(forest_cv.best_params_, forest_cv.best_score_)\n",
    "\n",
    "# 改めて最適パラメータで学習\n",
    "forest_reg = RandomForestRegressor(**forest_cv.best_params_)\n",
    "forest_reg.fit(train, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---\n",
    "# xgboostモデルの作成\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "xgb_cv = GridSearchCV(xgb_model, {'max_depth': [2,4,6], 'n_estimators': [50,100,200]}, verbose=1)\n",
    "xgb_cv.fit(train, y)\n",
    "print(xgb_cv.best_params_, xgb_cv.best_score_)\n",
    "\n",
    "# 改めて最適パラメータで学習\n",
    "xgb_reg = xgb.XGBRegressor(**xgb_cv.best_params_)\n",
    "xgb_reg.fit(train, y)\n",
    "#---\n",
    "\n",
    "\n",
    "blend_models_predict = ( (0.25 * skl.predict(test)) \n",
    "                        + (0.25 * las_reg.predict(test))\n",
    "                        + (0.25 * xgb_reg.predict(test))\n",
    "                        + (0.25 * forest_reg.predict(test)) )\n",
    "                        \n",
    "\n",
    "result = np.exp(blend_models_predict)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_id,\n",
    "    \"SalePrice\": result\n",
    "})\n",
    "submission.to_csv('gs://sample_machine_learning_output/HousePrices/hp_submission8.csv', index=False)\n",
    "########\n",
    "elapsed_time = time.time() - start\n",
    "print (\"経過時間:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "print(\"--End--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
